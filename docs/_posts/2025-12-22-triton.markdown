---
layout: post
title:  "Triton"
date:   2025-12-22 11:18:26 -0800
categories: CUDA
typora-root-url: ..
mathjax: true
---

![image-20251225144245290](/assets/images/triton-like-pytorch.png)

![image-20251225144329395](/assets/images/pytorch-like-triton.png)

## Future Work
https://pytorch.org/blog/warp-specialization-in-triton-design-and-roadmap/

## äºŒå…«å®šå¾‹
å·´è±ç‰¹å®šå¾‹ï¼ˆPareto Principleï¼‰ï¼Œåˆç§°**äºŒå…«å®šå¾‹**æˆ–**å¸•ç´¯æ‰˜æ³•åˆ™**ï¼Œæ˜¯ç”±æ„å¤§åˆ©ç»æµå­¦å®¶å¸•ç´¯æ‰˜å‘ç°çš„ï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯ï¼šåœ¨ä»»ä½•ä¸€ç»„äº‹ç‰©ä¸­ï¼Œæœ€é‡è¦çš„éƒ¨åˆ†ï¼ˆå…³é”®å°‘æ•°ï¼‰åªå çº¦20%ï¼Œå´èƒ½å¸¦æ¥80%çš„æˆæœï¼ˆä¸»è¦ç»“æœï¼‰ï¼Œè€Œå‰©ä½™80%çš„æ¬¡è¦éƒ¨åˆ†ï¼Œä»…å¸¦æ¥20%çš„æˆæœã€‚æ­¤å®šå¾‹å¹¿æ³›åº”ç”¨äºç»æµã€ç®¡ç†ã€æ—¶é—´ç®¡ç†ç­‰é¢†åŸŸï¼Œå¼ºè°ƒåº”èšç„¦äºé‚£å…³é”®çš„20%æŠ•å…¥ï¼Œä»¥è·å¾—ä¸»è¦çš„80%äº§å‡ºã€‚ 

**ä¸»è¦å†…å®¹**

- **å…³é”®å°‘æ•° vs. æ¬¡è¦å¤šæ•°**ï¼š20%çš„å…³é”®å› ç´ ï¼ˆå¦‚æ ¸å¿ƒå®¢æˆ·ã€é‡è¦ä»»åŠ¡ã€æ ¸å¿ƒäº§å“ï¼‰å¯¹æ•´ä½“ï¼ˆ80%çš„æ•ˆç›Šï¼‰èµ·å†³å®šæ€§ä½œç”¨ã€‚
- **éå‡è¡¡æ€§**ï¼šå¤§å¤šæ•°ç»“æœå¹¶éå¹³å‡åˆ†é…ï¼Œè€Œæ˜¯å‘ˆç°å‡ºæ˜¾è‘—çš„ä¸å¹³è¡¡çŠ¶æ€ã€‚ 

**åº”ç”¨é¢†åŸŸä¸å®ä¾‹**

- **ä¼ä¸šç®¡ç†**ï¼š20%çš„å®¢æˆ·è´¡çŒ®80%çš„åˆ©æ¶¦ï¼›80%çš„é”€å”®é¢æ¥è‡ª20%çš„å•†å“ã€‚
- **æ—¶é—´ç®¡ç†**ï¼š80%çš„æˆæœæºäº20%çš„æœ‰æ•ˆæ—¶é—´æŠ•å…¥ã€‚
- **èµ„æºåˆ†é…**ï¼šå°†80%çš„èµ„æºæŠ•å…¥åˆ°èƒ½äº§ç”Ÿ80%æ•ˆç›Šçš„20%çš„å…³é”®é¢†åŸŸã€‚
- **ä¸ªäººå‘å±•**ï¼šä¸“æ³¨äº20%çš„æ ¸å¿ƒæŠ€èƒ½æˆ–çŸ¥è¯†ï¼Œå®ç°80%çš„è¿›æ­¥ã€‚
- **ç¤¾ä¼šè´¢å¯Œ**ï¼šæ—©æœŸè§‚å¯Ÿåˆ°20%çš„äººå£æŒæ¡äº†80%çš„è´¢å¯Œã€‚ 

**å®é™…æ„ä¹‰**
å·´è±ç‰¹å®šå¾‹æŒ‡å¯¼äººä»¬è¯†åˆ«å’Œèšç„¦äºâ€œå…³é”®å°‘æ•°â€ï¼Œé¿å…åœ¨æ¬¡è¦çš„80%ä¸Šè€—è´¹è¿‡å¤šç²¾åŠ›ï¼Œä»è€Œæ›´é«˜æ•ˆåœ°åˆ†é…æœ‰é™çš„èµ„æºï¼Œä»¥æœ€å°çš„æŠ•å…¥è·å¾—æœ€å¤§çš„å›æŠ¥ã€‚

https://en.wikipedia.org/wiki/Pareto_front

## Tritonæ€»è§ˆ

**CUDA vs Triton** (ä¾‹å­ï¼šå‘é‡åŠ )

```c++
__global__ void add_kernel(float* a, float* b, float* c, int n) {
    // 1. Manually calculate the global index of this specific thread
    int i = blockIdx.x * blockDim.x + threadIdx.x;

    // 2. Add a boundary check (if the vector isn't a perfect multiple of block size)
    if (i < n) {
        c[i] = a[i] + b[i];
    }
}
```

```c++
import triton
import triton.language as tl

@triton.jit
def add_kernel(a_ptr, b_ptr, c_ptr, n, BLOCK_SIZE: tl.constexpr):
    # 1. Get the starting point for this "program" (tile)
    pid = tl.program_id(0)
    
    # 2. Create a vector of offsets (e.g., [0, 1, 2, ... 1023])
    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)
    
    # 3. Use a mask for boundary conditions (instead of an 'if' statement)
    mask = offsets < n
    
    # 4. Load, Add, and Store the entire tile at once
    a = tl.load(a_ptr + offsets, mask=mask)
    b = tl.load(b_ptr + offsets, mask=mask)
    output = a + b
    tl.store(c_ptr + offsets, output, mask=mask)
```

1. Triton æ˜¯ä¸€ç§ä¸­é—´è¯­è¨€åŠç¼–è¯‘å™¨ï¼Œå…¶è®¾è®¡åˆè¡·æ˜¯é€šè¿‡å®ç°**åŸºäºåˆ†å—çš„ç¥ç»ç½‘ç»œè®¡ç®—**çš„é«˜æ•ˆå¯ç§»æ¤æ–¹æ¡ˆï¼Œçªç ´ç°æœ‰å‚å•†åº“åœ¨æ–°å‹æ·±åº¦å­¦ä¹ ç®—å­ä¸Šçš„å±€é™æ€§ã€‚
2. Triton é‡‡ç”¨åˆ†å—æ¶æ„ï¼ˆè¿™é‡Œçš„ â€œåˆ†å—â€ æŒ‡å¼ é‡ä¸­ä¸€ä¸ªå°å‹çš„çŸ©å½¢å­åŒºåŸŸï¼‰ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒXLA åŸºäºå®Œæ•´å¼ é‡è¿è¡Œï¼Œå¼€å‘è€…æ— æ³•å¯¹å…¶å­å¼ é‡è¿›è¡Œæ“ä½œã€‚
3. Tritonæ˜¯ä¸€ç§å–èˆ :  portability / productivity / efficiency
4. Triton å¼€åˆ›äº†Tileç¼–ç¨‹ï¼Œä½†å’ŒCUDAä¸€æ ·ï¼Œè¿˜æ˜¯SPMD
   ![download](./assets/images/download.png)
5. Tritonçš„æ€§èƒ½é¢„æœŸï¼šé«˜ä¸‹é™ï¼Œä½ä¸Šé™ï¼Œä½æŒ‡ > 80% SoL
6. Tritonæ”¯æŒçš„ç¡¬ä»¶: GPU, CPU, Accelerator


![Screenshot 2025-12-25 at 2.26.44â€¯PM](/assets/images/Screenshot 2025-12-25 at 2.26.44â€¯PM.png)

<img src="/assets/images/triton-vs-cuda.png" alt="img" style="zoom: 67%;" />

## Tritonçš„æœªæ¥ç›®æ ‡

å¦‚æœè¯´æŸä¸€æ¨¡å‹çš„è¿è¡Œæ•ˆç‡è¾¾åˆ° â€œç†è®ºæ€§èƒ½ä¸Šé™çš„ 80%â€ï¼Œæ„æ€å°±æ˜¯è¯¥æ¨¡å‹å¯¹åº”çš„ä»£ç ï¼Œå®ç°äº†ç¡¬ä»¶ï¼ˆå¦‚è‹±ä¼Ÿè¾¾ H100 æ˜¾å¡ï¼‰ç‰©ç†å±‚é¢å¯æ‰¿è½½çš„æœ€å¤§ç†è®ºååé‡çš„ 80%ã€‚

![Screenshot 2025-12-24 at 6.52.37â€¯PM](/assets/images/Screenshot 2025-12-24 at 6.52.37â€¯PM.png)

| 2024 | 2025 |
| :---: | :---: |
| <img src="/assets/images/Screenshot 2025-12-24 at 9.04.33â€¯PM.png" alt="Screenshot 2025-12-24 at 9.04.33â€¯PM" style="zoom:67%;" /> | <img src="/assets/images/Screenshot 2025-12-24 at 9.02.38â€¯PM.png" alt="Screenshot 2025-12-24 at 9.02.38â€¯PM" style="zoom:67%;" /> |


## Triton-C / Triton-Python

- **Tile Declarations**: Special syntax for multi-dimensional arrays, e.g., `int tile[16, 16]`. Tile shapes are constant but can be tunable for compiler optimization.
- **Built-in Functions**: `dot`, `trans` for matrix operations, and `get_global_range(axis)` for retrieving thread-block global indices.
- Broadcasting: Numpy-like broadcasting semantics using `newaxis` and slicing (e.g., `range[:, newaxis]`).
- **Predication**: Basic control-flow within tile operations is achieved through predicated statements using the `@` prefix, allowing partial guarding of tile-level loads.

The semantics of Triton-C provide:

- **Tile Semantics**: Abstract away low-level performance details like memory coalescing, cache management, and specialized hardware utilization, allowing the compiler to optimize them automatically.
- **Broadcasting Semantics**: A two-step process: (1) padding the shorter operand with ones until dimensionalities match, and (2) replicating content to match shapes.
- **SPMD Programming Model**: Similar to CUDA, but each kernel is single-threaded and automatically parallelized, operating on a set of `get_global_range(axis)` values. This simplifies kernels by removing explicit CUDA-like concurrency primitives.




## Triton IR

Triton-IR is an LLVM/MLIR-based IR designed for tile-level program analysis, transformation, and optimization. It shares LLVM-IR's module, function, and basic block structure, utilizing Static Single Assignment (SSA) form. Its key extensions include:

- **Tile Types**: Multi-dimensional types, e.g., `i32<8,8>`, represent tiles. Parametric tunable shapes from Triton-C are resolved by the JIT compiler.
- **Tile-Level Data-Flow Instructions**:
  - Retiling Instructions: reshape creates a tile of specified shape from input data (useful for re-interpreting variables), and broadcast replicates an input tile along unit dimensions.
  - Scalar instructions (e.g., cmp, add, load) are extended to signify element-wise operations on tile operands.
  - Specialized arithmetic instructions: trans (transposition) and dot (matrix multiplication).
- **Tile-Level Control-Flow Analysis**: To handle divergent control flow within tiles (where branching is not feasible), Triton-IR introduces **Predicated SSA** (PSSA) and \\(\psi\\)-functions:
    - *cmpp* instruction: Similar to cmp, but returns two opposite boolean predicates.
    - *psi* instruction: Merges values from different streams of predicated instructions, ensuring correctness when operations are conditionally applied to tile elements.
    

## ç¼–è¯‘Passes

| ç¼–è¯‘è¿‡ç¨‹ | ç¼–è¯‘pipeline |
| :---: | :---: |
| <img src="/assets/images/triton-compiler.png" alt="img"  /> | <img src="./assets/images/Untitled-2025-10-24-1743.png" alt="Untitled-2025-10-24-1743" style="zoom: 33%;" /> |


## ç¼–è¯‘Passes (è¯¦ç»†)
<img src="/assets/images/triton-compiler2.png" alt="img"  />

The Triton-JIT compiles and optimizes Triton-IR programs into efficient machine code through a series of passes and an auto-tuning engine:

- **Machine-Independent Passes**:
**Pre-Fetching**: Automatically detects loops and inserts prefetching code to hide memory latency.
**Tile-Level Peephole Optimization**: Simplifies chains of tile operations using algebraic properties (e.g., $$X = (X^T)^TX=(XT)TX = (X^T)^TX=(XT)T)$$.

- **Machine-Dependent Passes**: These passes are designed for GPU architectures with hierarchical memory.
  - **Hierarchical Tiling**: Decomposes tiles into micro-tiles and nano-tiles to map computations optimally to the hardware's compute capabilities and memory hierarchy. The structure of Triton-IR allows automatic enumeration and optimization of nested tiling configurations.
  - **Memory Coalescing**: Orders threads within micro-tiles to ensure adjacent threads access nearby memory locations, reducing memory transactions to DRAM.
  - **Shared Memory Allocation**: Determines when and where to store tile operands in fast shared memory by analyzing variable live ranges and employing a linear-time allocation algorithm.
  - **Shared Memory Synchronization**: Automatically inserts barriers in the generated GPU code to preserve program correctness by detecting Read-After-Write (RAW) and Write-After-Read (WAR) hazards using forward data-flow analysis with equations:

    

- **Auto-tuner**: Optimizes meta-parameters associated with the optimization passes (e.g., tiling parameters). In this work, it uses an exhaustive search over powers of two for tile, micro-tile, and nano-tile sizes.

## æ—©æœŸæŒ‘æˆ˜

The main challenge posed by our proposed paradigm is that of work scheduling, i.e., how the work done by each program instance should be partitioned for efficient execution on modern GPUs. To address this issue, the Triton compiler makes heavy use of *block-level data-flow analysis*, a technique for scheduling iteration blocks statically based on the control- and data-flow structure of the target program. The resulting system actually works surprisingly well: our compiler manages to apply a broad range of interesting optimization automatically (e.g., **automatic coalescing, thread swizzling, pre-fetching, automatic vectorization, tensor core-aware instruction selection, shared memory allocation/synchronization, asynchronous copy scheduling**). Of course doing all this is not trivial; one of the purposes of this guide is to give you a sense of how it works.

## å½“å‰æŒ‘æˆ˜ Pipelining

Hand-fused loops in matmul kernel, multitude of functional and performance problems
â— Matmul optimized path not ready for branching in the
main loop
	â–  WGMMA pipelining ğŸ’€
	â–  Layout conversion optimizations ğŸ˜µ
	â–  Axis Analysis ğŸª¦

â— IfOp placement in the loop - major performance problem
	â–  Placed in the middle of the loop
	â–  Instruction scheduling off the window
	â–  Introduced CoarseSchedule, allowing for better control over ops placement

https://drive.google.com/file/d/1NOZ2LIZgHs3UQKCEw3Nv5rrChlX2bzZ0/view?usp=drive_link
https://www.youtube.com/watch?v=PAsL680eWUw


## [CPU Support](https://www.youtube.com/watch?v=obGM7nujV00)

â— OSS demands for computers without GPU

â— Internal business demands

â€‹	  â—‹ Small batch jobs, PyTorch for Edge (20% efforts for 80% perf) 



Compiler Overview ([third_party/cpu/backend/compiler.py](https://github.com/triton-lang/triton-cpu/blob/e60f448f8f197073b75d6d3e77347414a5db3ee7/third_party/cpu/backend/compiler.py#L109)) 

â–ª make_ttir: high-level optimizations (same with GPU) 
â–ª make_ttcir: TTIR â†’ TTCIR, lowering to vector dialect 
â–ª make_tttcir: TTCIR â†’ Target-specific TTCIR (TTTCIR) 
â–ª make_llir: Target-specific TTCIR â†’ LLVM IR 
â–ª make_asm: LLVM IR to .asm (static compilation) 
â–ª make_so: Compile .asm into .so 
â–ª The CPU driver and OpenMP launcher

## [MTIA Support](https://docs.google.com/presentation/d/1Cd-X30A7c4sdjoK20GdEsDV3qHm9jglD/edit?usp=drive_link&ouid=109270351505023978769&rtpof=true&sd=true)

Metaçš„åé¦ˆï¼š

- Need for improved support for memory subsystems
- **Need for improved support for asynchronous execution**
- Need for supporting cross-PE primitives and PE topology-aware codegen for custom HW targets

## [Hopper Support](https://docs.google.com/presentation/d/1BH509Am6JCbS5-CpqpQ1YBcZ-nunaODb/edit?usp=drive_link&ouid=109270351505023978769&rtpof=true&sd=true)



![Screenshot 2025-12-25 at 10.43.53â€¯AM](/assets/images/Screenshot 2025-12-25 at 10.43.53â€¯AM.png)

![Screenshot 2025-12-24 at 9.07.39â€¯PM](/assets/images/Screenshot 2025-12-24 at 9.07.39â€¯PM.png)

### **Hopper added a lot of complexity, so would Ascend**

**memory coalescing**: Orders threads within micro-tiles to ensure adjacent threads access nearby memory locations, reducing memory transactions to DRAM.
**prefetch**: prefetch from shared memory the operands (A and B) of a `tt.dot`, when this operation is located in a loop.
**reorder_instructions**: reorder instructions so as to (1) decrease register pressure (e.g., by moving conversions from shared memory before their first use) and (2) promote LLVM instruction order more friendly to `ptxas`.


https://github.com/triton-lang/triton/blob/167bdc81e7b563433fa65980eed4f609a727c74e/include/triton/Dialect/TritonGPU/Transforms/Passes.td

![Screenshot 2025-12-25 at 10.39.22â€¯AM](/assets/images/Screenshot 2025-12-25 at 10.39.22â€¯AM.png)

[`make_block_ptr`](https://github.com/triton-lang/triton/pull/1392): a new semantics: **block pointer**, which makes users easier & faster to load a block from a parent tensor.

![image-20251225113807245](/assets/images/image-20251225113807245.png)



## Blackwell Support & Gluon

### ä¸ºä»€ä¹ˆéœ€è¦Gluon

**Why**:  [achieving >80% SoL with the Triton middle-end is becoming intractable:](https://github.com/triton-lang/triton/issues/7392) (**Flash Attention on Blackwell is the motivating case for inventing Gluon**)

- Kernels are becoming more complex
- Hardware is gaining more degrees of freedom
- As the tensor core programming becomes increasingly complex: **warp-specialization, async MMA, async TMA, it becomes very hard for triton compiler to find a optimal schedule to maximize performance.** Hence more control of schedule should be exposed to the deveropers so that they could express the schedule easily, then triton compiler could do the rest.

Concrete problems:
- Optimal tensor layout assignment in registers and in memory
- Warp specialization partitioning and loop scheduling
- Clever synchronization tricks to squeeze more loop stages



### Gluonå’ŒTritonçš„å¼‚åŒ

* ç›¸åŒï¼šä¿æŒtileç¼–ç¨‹æ¨¡å‹ï¼Œä¿æŒSPMD
* ä¸åŒï¼šGluonå¯¹å¼€å‘è€…å¼€æ”¾æ§åˆ¶æƒ:  1. å¸ƒå±€ 2. è°ƒåº¦ 3. å†…å­˜

### Gluonç®€ä»‹

**æš‚æ—¶ä¸æ”¯æŒGluonå’ŒTritonæ··åˆç¼–ç¨‹ (as of 2025).** (I think it's the same situation as "C++ with inline assembly", need a good use-case and then define a clera boundary, then it should work)

Gluon is a GPU programming language based on the same compiler stack as Triton.
But unlike Triton, Gluon is a lower-level language that gives the user more
control and responsibility when implementing kernels.

At a high level, Gluon and Triton share many similarities. Both implement a
tile-based SPMD programming model, where tiles represent N-dimensional arrays
distributed over a "program". Both are Python DSLs sharing the same frontend
and JIT infrastructure.

Triton, however, abstracts many details of implementing kernels and GPU hardware
from the user. It defers to the compiler to manage tile layouts, memory
allocation, data movement, and asynchronity.

Getting these details right is important to kernel performance. While the Triton
compiler does a good job of generating efficient code for a wide range of
kernels, it can be beaten by hand-tuned low-level code. When this happens,
there is little the user can do to significantly improve performance since all
the details are hidden.

In Gluon, these details are exposed to the user. This means writing Gluon
kernels requires a deeper understanding of GPU hardware and the many aspects of
GPU programming, but it also enables writing more performant kernels by finely
controlling these low-level details.

**[Gluon has language support for warp-specialization](https://github.com/triton-lang/triton/blob/a85ba297726135181e0b4bb202e25b4d2c9a3ebc/python/tutorials/gluon/08-warp-specialization.py#L267-L284).**

#### Linear Layouts

Triton linear layout is target-independent. Before, Triton's layout representation is target-dependent.

**Gluon vs Cutlass cute**

> While both CUTE and linear layouts aim to address the challenge of flexible task mapping on emerging architectures, they differ in several key aspects. First and foremost, CUTE is primarily designed for users to manually describe layouts, whereas linear layouts are integrated into a compiler. Second, the linear algebra framework of linear layouts enables compilers to generate efficient code for layout conversion and code lowering for many common operators, which is absent in CUTE. Third, swizzling is inherently defined within linear layouts, whereas in CUTE, it is treated as a separate step

https://www.lei.chat/posts/triton-compiler-development-tips/

## [Liger Kernel](https://pytorch.org/blog/peak-performance-minimized-memory/)
Liger Kernel is an open source library of optimized Triton kernels designed to enhance the efficiency and scalability of training Large Language Models (LLMs). It focuses on kernel-level optimizations such as operation fusing and input chunking, achieving significant improvements in training throughput and GPU memory usage compared to existing implementations like those from HuggingFace. By using a single line of code, Liger Kernel can improve training throughput by 20% and reduce memory usage by 60%.

**NOTE** Trition is expressive enough to have operation fusing and input chunking optimization.

## [Triton-Distributed](https://triton-distributed.readthedocs.io/en/latest/)

TODO

## [Profiling](https://youtu.be/PGUw2P55ZYM?si=fawMOhJq-BwTO9DR)
Proton(profiling for Triton)

## æ€»ç»“

Triton çš„æµæ°´ç¼–æ’æ§åˆ¶ä¸å¤Ÿã€‚å¦‚æœè¦ä¿æŒå½“å‰çš„åº”ç”¨æ€§ï¼Œè¦ä¹ˆç¡¬ä»¶éœ€è¦åŠ æ›´å¤šçš„æµæ°´è¾…åŠ©èƒ½åŠ›ã€‚è¦ä¹ˆå°±è¦è®©Tritonçš„æŠ½è±¡å±‚æ¬¡ä½ä¸€äº›æš´éœ²æ›´å¤šæ§åˆ¶çš„èƒ½åŠ›ã€‚

Triton has trouble maintaining both 80% SoL and portability 

Triton does not support multi-CTA well (DSMEM, Hopper/Blackwell) 

Triton/Gluon composition vs C++/Assembly composition


user want FAST AND EASY gpu programming language. BOTH FAST AND EASY; For a specific task, user decide what's kind of performance is acceptable, then they want to reach that performance goal ASAP. FAST first, then trade productivity for  < 20% perf drop is acceptable. 

80% SoL (AKA 80/20 rule matters for prototying and Triton's main use case is prototyping, for example, [flash-linear-attention](https://github.com/fla-org/flash-linear-attention)) 

Ascend will have the same problem as Hopper / Blackwell: precise control is needed to have 80% SoL

Autotuning is important for Ascend

Debugging experiences are important (python/AST/MLIR/LLVMIR/ELF, too many layers to get debuginfo right)

People like Python or DSL. Adopt Python-first strategy instead of "we support Python"

Attention-oriented (or whatever ops trending LLM needs), because that's the most important use case

Windows-support



## å‚è€ƒèµ„æ–™

https://pytorch.org/blog/triton-kernel-compilation-stages/
