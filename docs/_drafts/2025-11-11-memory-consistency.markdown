---
layout: post
title:  "CUDA Memory Consistency Model"
# date:   2025-11-11 11:18:26 -0800
categories: CUDA
typora-root-url: ..
---



We explain these points in order. First, a `wgmma.fence` instruction ensures that `wgmma.mma_async` only accesses certain RMEM addresses after all prior accesses to such addresses have finished. Without the `wgmma.fence`, the behavior is undefined. An exception to this rule is that Hopper allows *multiple* `wgmma.mma_async` instructions to be in flight simultaneously. As long as these `wgmma.mma_async` instructions have the same accumulator shape, they can share the same accumulator tensor, i.e., write to the same register memory addresses. In that case, a fence is not required. For example, we don‚Äôt need to insert a `wgmma.fence` within the loop over `MMA_K` done as part of the `cute::gemm` call.



```cpp
cute::warpgroup_arrive();
cute::gemm(tiled_mma, tCrA(_,_,_,read_pipe), tCrB(_,_,_,read_pipe), tCrC);
cute::warpgroup_commit_batch();
cute::warpgroup_wait<0>();
```





Below is a clear, accurate, up-to-date summary of the Blackwell (SM100) and Hopper (SM90) async-proxy memory model rules, based on the PTX ISA 8.x/9.x updates and NVIDIA‚Äôs memory model papers.

This is the definitive explanation of async-proxy fences, TMA, and how async engines interact with CUDA‚Äôs scoped memory model.

‚∏ª

‚ö°Ô∏è Background: What is an async proxy?

Starting with Hopper (SM90) and continuing in Blackwell (SM100), some memory operations are executed by async engines that do not participate in normal GPU memory ordering rules.

This includes:

Hopper (SM90)
	‚Ä¢	TMA (Tensor Memory Accelerator): async global‚Üíshared copies
	‚Ä¢	GMMA: tensor core loads
	‚Ä¢	cp.async.bulk.tensor / tma.load
	‚Ä¢	cp.async.cg.shared.global.phys ‚Üí global‚Üíshared async pipeline
	‚Ä¢	Async barriers (mbarrier) interacting with async copy engines

Blackwell (SM100)
	‚Ä¢	UMMA (Unified Multi-Modal Accelerator): async local/remote memory engines
	‚Ä¢	Async data movement within tensor memory space
	‚Ä¢	More async hardware paths that do not obey PTX standard .acq/.rel scopes unless fenced

These engines move data outside the normal memory consistency pipeline ‚Üí hence ‚Äúproxy.‚Äù

‚∏ª

üß† Why async-proxy rules exist

GPU cores have a memory model defined by:
	‚Ä¢	Scopes: .cta, .gpu, .sys
	‚Ä¢	Orders: .acq, .rel, .sc, .weak

But:
	‚Ä¢	Async engines (TMA/UMMA) do not obey these orders.
	‚Ä¢	They can read/write memory without ordering guarantees relative to normal ld/st, atom, etc.

Thus, NVIDIA introduced proxy-memory operations and fences.

‚∏ª

üöß Hopper & Blackwell: Async Proxy Memory Model Rules

1. Async operations are not ordered w.r.t. normal PTX loads/stores

Example:

tma.load ...
st.global ...

There is no guarantee that the TMA completes before or after the st.global.

To order them, you must use a proxy fence.

Equivalent for:
	‚Ä¢	cp.async.bulk.tensor
	‚Ä¢	mma.load (tensor core async)
	‚Ä¢	Blackwell UMMA load/store ops

‚∏ª

2. The floor of ordering: fence.proxy and mbarrier.proxy

PTX introduces:

Hopper (SM90):

fence.proxy.async
fence.proxy.tensormem
mbarrier.proxy

Blackwell (SM100) adds new proxy types:

fence.proxy.umba       // UMMA engines
fence.proxy.tma        // Tensor Memory Accelerator
fence.proxy.cluster
... and combinations

These fences establish happens-before edges between async engines and core memory model.

Meaning:

st.global.relaxed
fence.proxy.async
... async engine sees the writes ...

Without this fence: async engines may observe stale data.

‚∏ª

3. All async storage targets have separate proxy domains

Hopper domains:
	‚Ä¢	async
	‚Ä¢	tensormem
	‚Ä¢	generic

Blackwell extends this:
	‚Ä¢	umma (new async memory engines)
	‚Ä¢	tma (old TMA becomes its own domain)
	‚Ä¢	cluster-proxy (for cluster shared memory)
	‚Ä¢	tensorcore-proxy (GMMA pathways)

Comparing domains is key:
	‚Ä¢	Operations in different proxy domains are not ordered unless fenced.

‚∏ª

üîÑ Example: Hopper TMA sync with shared memory

tma.load.tensor.shared::cta.desc ...
mbarrier.arrive.expect_tx ...
mbarrier.wait ...
fence.proxy.tensormem
ld.shared ...

Explanation:
	1.	tma.load kicks off async copy into shared memory.
	2.	mbarrier.wait ensures copy completed, but does not order memory wrt. SM core.
	3.	fence.proxy.tensormem is required to make SM core see newly-written data.

Without fencing: SM may read stale or partially-written shared memory.

‚∏ª

üß± Hopper/Blackwell Async Memory Model Rules (Textbook summary)

Rule A ‚Äî Async engines do NOT participate in CUDA‚Äôs memory model.

They are ‚Äúoutside‚Äù the .acq/.rel/.sc ordering rules.

Rule B ‚Äî Async-copy writes become visible only after an appropriate proxy fence.

Rule C ‚Äî Barriers (mbarrier) handle completion but NOT memory visibility ordering.

Completion is distinct from visibility.

Rule D ‚Äî Proxy domain fences must match the async operation type.

Wrong domain ‚Üí no synchronization.

Rule E ‚Äî Cross-proxy ordering requires two fences:
	‚Ä¢	One releases from producer domain
	‚Ä¢	One acquires in consumer domain

‚∏ª

üì¶ Blackwell-specific updates (beyond Hopper)

Blackwell (PTX ISA 9.x) introduces:

1. UMMA async instructions
	‚Ä¢	They operate in their own proxy domain (proxy.umma)
	‚Ä¢	Require fence.proxy.umma for visibility ordering

2. Cluster Tensor Memory (CTM) domain

For distributed shared memory across SMs in a cluster.

3. More explicit proxy categories

E.g., Blackwell distinguishes:
	‚Ä¢	proxy.tma
	‚Ä¢	proxy.umba
	‚Ä¢	proxy.cluster
	‚Ä¢	proxy.tensormem

Hopper treated TMA/TensorCore more implicitly.

4. Proxy+scope combined fences

New forms like:

fence.sc.proxy.tma.gpu
fence.acq_rel.proxy.umma.cta

These create explicit HB edges between async pathways and normal memory across scopes.

‚∏ª

üß© Simplified mental model

Normal CUDA memory model = CPU-like acquire/release + scopes.
Async engines = DMA engines / tensor loaders outside the model.
Proxy domains = isolated worlds that must be connected with fences.

normal core ops
   |
   |  (barrier / mbarrier)
   |
async engine ops

Fences wire them together.

‚∏ª

‚ú® Want diagrams or examples?

I can provide:

‚úÖ diagrams showing proxy domains
‚úÖ litmus tests demonstrating incorrect behavior without proxy fences
‚úÖ a Blackwell-to-Hopper comparison table
‚úÖ examples from TMA, UMMA, and GMMA pipelines
‚úÖ CUTLASS pipelines explained with proxy rules

Tell me what you‚Äôd like!    `Sadfsda`



## References

- https://github.com/NVlabs/mixedproxy
- [GPU Concurrency: Weak Behaviours and Programming Assumptions](https://dl.acm.org/doi/10.1145/2775054.2694391)
- A Formal Analysis of the NVIDIA PTX Memory Consistency Model
- https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#memory-synchronization-domains
- [PTX Memory Consistency Model](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#memory-consistency-model)