---
layout: post
title:  "Tensor Memory Accelerator (TMA)"
# date:   2025-11-19 11:18:26 -0800
categories: CUDA
typora-root-url: ..
---

## Problem Statement




In historical context, these developments continue a trend of replacing general-purpose computational resources by specialized hardware resources, to both remove bottlenecks and free up those general-purpose resources for other operations. Starting with the Volta architecture, the Tensor Cores divorced GEMM arithmetic operations from the general computational pipeline. Ampere’s asynchronous copy instructions allowed for true pipelining of GEMM mainloops. On Hopper GPUs, the asynchronous, single-threaded TMA and the ability to reallocate registers between warpgroups dramatically reduced the register and thread cost of data movement, and the asynchronous WGMMA allowed for pipelining of MMA with other compute operations. Now, Tensor Memory and UMMA do for MMA just what TMA did for copy, making it a single-threaded, asynchronous operation that does not consume registers. As a result, registers can primarily be used for other tasks like scheduling and fused epilogue operations.

**大的背景是把专有算力和通用算力在架构上分离，从而达到提高性能的目的。**



## Methodology

算存比，提高内存带宽利用率，需要用到异步来overlap计算和拷贝

## [Volta](https://developer.nvidia.com/blog/cutlass-linear-algebra-cuda/#software_pipelining)/[Ampere](https://developer.nvidia.com/blog/controlling-data-movement-to-boost-performance-on-ampere-architecture/) Software Pipelining

在Volta/Ampere架构上，Tensor Core还是依赖RMEM完成MMA指令（fragments, accumulators）。这样会带来寄存器压力，从而限制了occupancy。因此包含GEMM运算的kernel比不包含GEMM的kernel所能launch的warp要少很多，无法做到很好的warp之间的计算/拷贝遮掩。因此我们需要依赖软流水（software pipelining）来实现计算/拷贝的遮掩。Volta和Ampere的软流水都用下图的方式实现，区别在于在Volta上没有异步拷贝支持，需要更多依赖编译器的指令调度来实现遮掩；而在Ampere上，可以直接用异步拷贝实现遮掩。

![Figure 8. Three concurrent streams of instructions interleaved in the main loop of CUTLASS’s GEMM implementation. The orange arrows show data dependencies. While the memory system is loading data from global memory and the SM is loading fragments for the next thread tile, threads keep the SM busy by executing math instructions for the current tile.](https://developer.nvidia.com/blog/parallelforall/wp-content/uploads/2017/12/fig-10-software-pipeline-1.png)

[How does the LSU (Load/Store Unit) execute Load/Store instructions in the Ampere architecture?](https://forums.developer.nvidia.com/t/how-does-the-lsu-load-store-unit-execute-load-store-instructions-in-the-ampere-architecture/273699)

## Asynchrony


## [Warp Specialization](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#spatial-partitioning-also-known-as-warp-specialization)
Warp Specialization is introduced in Hopper.

A standard GPU program executes the same logic on each warp, while a warp specialized program uses different warps to execute different components of the overall program. Let’s take a look at some of these warp specialization strategies in the aforementioned contexts.

在一个warp内，warp divergence

- **[CUDA-DMA](https://lightsighter.org/pdfs/cudadma-sc11.pdf)**: separated the warps into memory loading (GMEM->SMEM) warps and compute warps; the loader warps issue loads and signal the compute warps when the loaded data is available.
- **[Singe compiler](https://cs.stanford.edu/~sjt/pubs/ppopp14.pdf)**: 
- **CUDA Tensor Core**: Specialized warps are used on Hopper and Blackwell to issue either TMA copies or Tensor Core matrix-multiplies. The TMA warp issue copies and notifies the Tensor Core warps when data is ready to be multiplied, and the Tensor Core warps notify the TMA warp when data has been consumed and the memory is free to use for more copies.
- **[high performance Flash Attention implementation on Blackwell](https://github.com/NVIDIA/cutlass/tree/a49a78ffefc86a87160dfe0ccc3a3a2d1622c918/examples/77_blackwell_fmha)**: uses at least 5 different kinds of specialized warps! In this Flash Attention implementation, there are warps for loading data, issuing matrix multiplication, computing softmax, scaling intermediate results, and storing data. As a result, the code is complex; the strategy itself is carefully constructed to yield high performance, and there is abundant cross-warp data movement and synchronization. Imagine the code above with 5 different warp cases and each cases signaling the others to proceed at different times! (多级流水：类似Asend NPU；有点像CPU流水线了？？)

With warp-specialization, some warps are dedicated to memory fetches (producers), while others are dedicated to compute (consumers), and named barriers are used for synchronization between them. The idea is that the warp schedulers can then more easily hide the latency of copy operations within compute (and vice-versa).

```cpp
if warpid() == LOAD:
  for i, tile in enumerate(tiles):
    if i > 0:
      wait_for_tile_release()
    async_tma_load(tile)
    wait_for_tma_load()
    signal_tile_loaded()
else:
  for tile in enumerate(tiles):
    wait_for_tile_loaded()
    tile_data = get_loaded_tile(tile)
    async_mma(tile_data)
    wait_for_async_mma()
    signal_tile_released()
```

![GEMM Pipeline](https://rohany.github.io/blog/warp-specialization/gemm-pipe.png)

- [Independent Thread Scheduling](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#independent-thread-scheduling)
- https://rohany.github.io/blog/warp-specialization/
- [Enabling advanced GPU features in PyTorch – Warp Specialization](https://pytorch.org/blog/warp-specialization/)
- https://www.cs.cmu.edu/~zhihaoj2/15-779/slides/06-warp-specialization.pdf
- [Part 1, this one] discusses the warpgroup matrix-multiply-accumulate (WGMMA) instructions. These are the primitive instructions that target the Tensor Core of NVIDIA GPUs based on the Hopper architecture.
  https://research.colfax-intl.com/cutlass-tutorial-wgmma-hopper/
  [Part 2] will discuss the overall design of an efficient GEMM kernel, including advanced techniques used in CUTLASS kernels such as warp-specialization and ping-pong scheduling.
  https://research.colfax-intl.com/cutlass-tutorial-design-of-a-gemm-kernel/
  [Part 3] will discuss persistent kernels and Stream-K, a load-balancing strategy for GEMM that achieves state-of-the-art efficiency across a vast number of problem geometries.
  https://research.colfax-intl.com/cutlass-tutorial-persistent-kernels-and-stream-k/

### [PTX setmaxnreg](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#miscellaneous-instructions-setmaxnreg) (Hopper & Blackwell)

正常情况下，编译器需要静态决定算子用的寄存器数量，这样会强制给producer和consumer分配同样数量的寄存器。但问题是producer一般是用TMA搬运数据，很少用到寄存器，但却被迫要和consumer用同样数量的寄存器。显然，这样会造成寄存器的浪费，因此在Hopper上，setmaxnreg指令可以在运行时让warp之间重新分配寄存器。用setmaxnreg指令把producer的寄存器转给consumer用，这样可以提高资源使用率。

```cpp
// Example values for Hopper GEMM with 1 consumer warpgroup
using LowerRegisterCount = Int<40>;
using HigherRegisterCount = Int<256>;
 
if (warp_group_idx == producerWarpGroupId) {
  cutlass::arch::warpgroup_reg_dealloc<LowerRegisterCount{}>();   // <============= dealloc from producer
  int lane_predicate = cute::elect_one_sync();
  if (warp_idx_in_warpgroup == 0 && lane_predicate) {
    PipelineState smem_pipe_write = 
      cutlass::make_producer_start_state<MainloopPipeline>();
    for (...) {
      pipeline.producer_acquire(smem_pipe_write);
      copy(...); // TMA
      ++smem_pipe_write;
    }
  }
} else { // consumer warpgroup
  cutlass::arch::warpgroup_reg_alloc<HigherRegisterCount{}>();   // <============= alloc to consumer
  PipelineState smem_pipe_read;
  for (...) {
    pipeline.consumer_wait(smem_pipe_read);
    gemm(...); // WGMMA
    pipeline.consumer_release(smem_pipe_read);
    ++smem_pipe_read;
  }
  // Epilogue to write out accumulator
  axpby(...);
}
```




### Hopper Warp Specialization
There are two pipelining strategies that are effective on the Hopper architecture:

- **Warp-specialization**. Specializing warps into producers (data transfer) and consumers (compute), and having them run concurrently.
- **Multistage**. Masking data transfer by using asynchronous copy (TMA on Hopper or cp.async on Ampere) to load the next set of data, while computing on the current set. Warps take on both producer and consumer roles.


![alt text](/assets/images/hopper-register.png)


### Blackwell Warp Specialization

## Pipelining
**The blocked structure demands a large storage allocation within the registers of each CUDA thread. The accumulator elements typically occupy at least half a thread’s total register budget. Consequently, occupancy – the number of concurrent threads, warps, and threadblocks – is relatively low compared to other classes of GPU workloads. This limits the GPU’s ability to hide memory latency and other stalls by context switching to other concurrent threads within an SM.**

To mitigate the effects of memory latency, CUTLASS uses software pipelining to overlap memory accesses with other computation within a thread. CUTLASS accomplishes this by **double buffering** at the following scopes.

- **Threadblock-scoped shared memory tiles**: two tiles are allocated in shared memory. One is used to load data for the current matrix operation, while the other tile is used to buffer data loaded from global memory for the next mainloop iteration.

- **Warp-scoped matrix fragments**: two fragments are allocated within registers. One fragment is passed to CUDA and TensorCores during the current matrix computation, while the other is used to receive shared memory fetch returns for the next warp-level matrix operation.

The following diagram illustrates the efficient, pipelined mainloop body used in CUTLASS GEMMs.


<!--
![alt text](assets/images/cutlass-mainloop.png)
-->


### cuda::pipeline
CUDA provides the cuda::pipeline synchronization object to manage and overlap asynchronous data movement with computation.

Pipeline Class Member Function: `producer_acquire`/`producer_commit`/`consumer_wait`/`consumer_release`



## Programming Model
### PTX
- cp.async (SM80, SM86, SM89)
- TMA (“Tensor Memory Accelerator”) async loads (SM90 / Hopper, SM100 / Blackwell)

TODO: examples

### CUTLASS
TODO: examples

### CUDA-C++
cuda::memcpy_async() (a C++20 <cuda/pipeline> API) launches an asynchronous DMA-like transfer from global memory (GMEM) to shared memory (SMEM).
The calling thread/warp does not stall.

TODO: examples
```cpp
#include <cuda/pipeline>

__global__ void kernel(float* gmem, float* smem) {
    cuda::pipeline<cuda::thread_scope_thread> pipe;

    // issue async copy
    cuda::memcpy_async(pipe, smem, gmem, 1024);

    // commit the transaction
    pipe.commit();

    // wait until data arrives in shared memory
    pipe.wait_prior<0>();

    // safe to read smem now
    float x = smem[0];
}
```





