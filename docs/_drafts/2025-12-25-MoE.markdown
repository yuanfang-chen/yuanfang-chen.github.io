---
layout: post
title:  "MoE"
# date:   2025-12-08 11:18:26 -0800
categories: CUDA
typora-root-url: ..
mathjax: true
---

## [MoE & Blackwell](https://youtu.be/GL7ImGZj-Oc)



## [mixture-of-experts](https://www.nvidia.com/en-us/glossary/mixture-of-experts/) (MoE)

https://huggingface.co/blog/moe



Grouped GEMM APIs
Grouped GEMM APIs can be viewed as a generalization of the batched APIs that enable different matrix sizes, transpositions, and scaling factors to be grouped and parallelized in one kernel launch.

One example where this approach provides speedup is the generation phase of a mixture-of-experts (MoE) model with batch sizes of 8 and 64 and FP16 inputs and outputs. In this example, the grouped GEMM API can achieve a 1.2x speedup over naive looping using the batched GEMM API. 


## References

- [Applying Mixture of Experts in LLM Architectures](https://developer.nvidia.com/blog/applying-mixture-of-experts-in-llm-architectures/)
- [Introducing Grouped GEMM APIs in cuBLAS and More Performance Updates](https://developer.nvidia.com/blog/introducing-grouped-gemm-apis-in-cublas-and-more-performance-updates/)



