



## Off-Policy vs On-Policy
In Deep Reinforcement Learning (RL), the distinction between **On-Policy** and **Off-Policy** comes down to one question: *Are you learning from what you are doing right now, or are you learning from a "recording" of past behavior?*

To understand this, we look at the interaction between the **Behavior Policy** (the one collecting data) and the **Target Policy** (the one being trained).

---

### 1. On-Policy Learning: "Learning on the Job"

In on-policy learning, the agent can only learn from data generated by its **current** version. It’s like a chef who only learns to cook by tasting the exact dish they are preparing at this very second.

* **The Rule:** Behavior Policy = Target Policy.
* **The Process:** You take an action, see the result, update your brain, and then **throw that data away**. You cannot use it again because your "brain" has changed, and the old data no longer represents what your *current* brain would do.
* **Key Advantage:** It is very **stable**. Because the agent is learning from its own direct experience, the updates are consistent and reliable.
* **Key Disadvantage:** It is **sample inefficient**. You need a massive amount of new data for every tiny update because you can't reuse old experiences.

**Famous Example:** **PPO** (Proximal Policy Optimization). It is the standard for training LLMs (RLHF) because it is stable and less likely to "crash" during training.

---

### 2. Off-Policy Learning: "Learning from History"

Off-policy learning decouples the "doing" from the "learning." The agent can look at a **Replay Buffer** (a database of past actions) and learn from them, even if those actions were taken by an older, "dumber" version of itself.

* **The Rule:** Behavior Policy  Target Policy.
* **The Process:** The agent saves every experience into a "memory bank." It then samples random memories to learn. It can even learn from "expert" data (like a human's gameplay) or random exploration.
* **Key Advantage:** Extremely **sample efficient**. It squeezes every bit of value out of every single interaction by re-studying it thousands of times.
* **Key Disadvantage:** It can be **unstable**. Because you are learning from a policy that is different from your current one, the math can sometimes "diverge," leading to the model failing or hallucinating bad strategies.

**Famous Example:** **DQN** (Deep Q-Network) or **SAC** (Soft Actor-Critic). These are often used in robotics or games where every single data point is "expensive" to collect.

---

### Summary Comparison

| Feature | On-Policy (e.g., PPO) | Off-Policy (e.g., DQN/SAC) |
| --- | --- | --- |
| **Data Usage** | Use once and discard | **Store and reuse (Replay Buffer)** |
| **Efficiency** | Low (Needs constant new data) | **High** (Learns from history) |
| **Stability** | **High** (Safe updates) | Lower (Prone to divergence) |
| **Source of Data** | Only the current model | Current model, old models, or humans |
| **Analogy** | Learning a sport by playing | Learning a sport by watching game film |

---

### Why this matters for the "Evaluator"

As we discussed earlier, the **Evaluator** in an off-policy system is often much more complex. Since the model is looking at "old" data, the evaluator has to use techniques like **Importance Sampling** to correct for the fact that the "old self" might have made mistakes that the "new self" wouldn't make.

**Would you like to know why PPO is "on-policy" even though it actually reuses data for a few mini-batches?**

[On-policy vs off-policy algorithms](https://www.youtube.com/watch?v=YUKUXoUg3Nc)
This video clearly explains the roles of the behavior and target policies, helping you visualize the fundamental architectural differences between these two RL approaches.




The agent’s decision-making process is called the policy π

The main goal of Reinforcement learning is to **find the optimal policyπ∗\*π\*∗ that will maximize the expected cumulative reward**. Because Reinforcement Learning is based on the *reward hypothesis*: **all goals can be described as the maximization of the expected cumulative reward.**



Three approaches to find optimal policy π:

- *Policy-based methods*(aka Policy Optimization): Train the policy directly to learn which action to take given a state.

- *Value-based methods*(aka Q-Learning): Train a value function to learn which state is more valuable and use this value function to take the action that leads to it.
  - For the state-value function, we calculate the value of a state
  - For the action-value function, we calculate the value of the state-action pair , hence the value of taking that action at that state
  
- *actor-critic* method, which is a combination of value-based and policy-based methods.





we use Monte-Carlo sampling to estimate return (we use an entire episode to calculate the return)



## Value-based methods

Monte Carlo and Temporal Difference Learning are two different strategies on how to train our value function or our policy function. 

## Policy-based methods

### The difference between policy-based and policy-gradient methods

Policy-gradient methods, what we’re going to study in this unit, is a subclass of policy-based methods. In policy-based methods, the optimization is most of the time *on-policy* since for each update, we only use data (trajectories) collected **by our most recent version of**πθ*π**θ*.

The difference between these two methods **lies on how we optimize the parameter**θ*θ*:

- In *policy-based methods*, we search directly for the optimal policy. We can optimize the parameterθ*θ* **indirectly** by maximizing the local approximation of the objective function with techniques like hill climbing, simulated annealing, or evolution strategies.
- In *policy-gradient methods*, because it is a subclass of the policy-based methods, we search directly for the optimal policy. But we optimize the parameterθ*θ* **directly** by performing the gradient ascent on the performance of the objective functionJ(θ)*J*(*θ*).

## Actor-Critic method

The solution to reducing the variance of the Reinforce algorithm and training our agent faster and better is to use a combination of Policy-Based and Value-Based methods: *the Actor-Critic method*.





Proximal Policy Optimization (PPO)



## [RLHF](https://huggingface.co/blog/rlhf)

### 



### [Direct Preference Optimization (DPO) ](https://huggingface.co/papers/2305.18290)

### Identity Preference Optimisation (IPO)

### Kahneman-Tversky Optimisation (KTO)

### Odds Ratio Preference Optimization (ORPO)

### Group Relative Policy Optimization (GRPO)

GRPO Trainer for training language models, as described in the paper [DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models](https://huggingface.co/papers/2402.03300) by [Zhihong Shao](https://huggingface.co/syhia), [Peiyi Wang](https://huggingface.co/peiyiwang89), [Qihao Zhu](https://huggingface.co/zqh11), Runxin Xu, [Junxiao Song](https://huggingface.co/haha-point),



## References

https://spinningup.openai.com/en/latest/spinningup/rl_intro.html

https://cameronrwolfe.substack.com/p/grpo

RLHF Deciphered: A Critical Analysis of Reinforcement Learning from Human Feedback for LLMs
