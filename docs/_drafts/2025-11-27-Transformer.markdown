---
layout: post
title:  "Transformer 笔记"
date:   2025-11-27 23:44:54 +0800
categories: ascend
typora-root-url: ..
---

## Feed-Forward Neural Networks



## Attention Mechanism





## Transformer结构

### Attention Layer

用来理解上下文

### Multilayer perceptron (MLP)

用来记忆





## 材料

- [3Blue1Brown - Transformers, the tech behind LLMs \| Deep Learning Chapter 5](https://www.youtube.com/watch?v=wjZofJX0v4M)
- [3Blue1Brown - Attention in transformers, step-by-step \| Deep Learning Chapter 6](https://www.youtube.com/watch?v=eMlx5fFNoYc)
- [3Blue1Brown - How might LLMs store facts \| Deep Learning Chapter 7](https://www.youtube.com/watch?v=9-Jl0dxWQs8)
- [What is Query, Key, and Value (QKV) in the Transformer Architecture and Why Are They Used?](https://epichka.com/blog/2023/qkv-transformer/)
- (must read) https://www.ibm.com/think/topics/attention-mechanism
- [Introduction to Attention Mechanism](https://erdem.pl/2021/05/introduction-to-attention-mechanism)
- [(attention mechanism) Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)
- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
- https://www.datacamp.com/tutorial/feed-forward-neural-networks-explained
