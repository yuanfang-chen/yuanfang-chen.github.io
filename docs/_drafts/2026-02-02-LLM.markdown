## Tokenization

The input to lanuage models are tokens. The process of converting training data to tokens is called tokenization.

After a tokenizer is initialized and trained, it is then used in the training process of its associated language model. This is why a pretrained language model is linked with its tokenizer and can’t use a different tokenizer without training.



## Token Embeddings

finding the best numerical representation for these tokens that the model can use to calculate and properly model the patterns in the text. These patterns reveal themselves to us as a model’s coherence in a specific language, or capability to code, or any of the growing list of capabilities we expect from language models.



In transformer-based models, the term "embedding" can refer to both static embeddings and dynamic contextual representations:

1. **Static Embeddings** generated in the first layer and combine token embeddings (vectors representing tokens) with positional embeddings (vectors encoding a token's position in the sequence).
2. **Dynamic Contextual Representations**. As input tokens pass through the self-attention and feed-forward layers, their embeddings are updated to become contextual. These dynamic representations capture the meaning of tokens based on their surrounding context. For example, the word "bank" appears both as "river bank" and "bank robbery", and while the **token embedding** of the word bank is the same in both cases, the transformations it goes through in the layers of the network take into account the context of which the word "bank" appears in.

![Diagram showing how embeddings fit into LLM architecture: input text is tokenized, converted to token embeddings, combined with positional embeddings, and processed through transformer layers to generate contextual representations.](./assets/images/llm_embedding_overview.png)

*Overview of how embeddings fit into the LLM architecture*



## Language Models

The language model holds an embedding vector for each token in the tokenizer’s vocabulary.

All the Transformer models mentioned above (GPT, BERT, T5, etc.) have been trained as *language models*. This means they have been trained on large amounts of raw text in a self-supervised fashion.

Self-supervised learning is a type of training in which the objective is automatically computed from the inputs of the model. That means that humans are not needed to label the data!

This type of model develops a statistical understanding of the language it has been trained on, but it’s less useful for specific practical tasks. Because of this, the general pretrained model then goes through a process called *transfer learning* or *fine-tuning*. During this process, the model is fine-tuned in a supervised way — that is, using human-annotated labels — on a given task.

An example of a task is predicting the next word in a sentence having read the *n* previous words. This is called *causal language modeling* because the output depends on the past and present inputs, but not the future ones.



## Types of language models

In the Transformers library, language models generally fall into three architectural categories:

1. **Encoder-only models** (like BERT): These models use a bidirectional approach to understand context from both directions. They’re best suited for tasks that require deep understanding of text, such as classification, named entity recognition, and question answering.
2. **Decoder-only models** (like GPT, Llama): These models process text from left to right and are particularly good at text generation tasks. They can complete sentences, write essays, or even generate code based on a prompt.
3. **Encoder-decoder models** (like T5, BART): These models combine both approaches, using an encoder to understand the input and a decoder to generate output. They excel at sequence-to-sequence tasks like translation, summarization, and question answering.



## [What is the difference between an autoencoder and an encoder-decoder?](https://datascience.stackexchange.com/questions/53979/what-is-the-difference-between-an-autoencoder-and-an-encoder-decoder)

Auto Encoders are a special case of encoder-decoder models. In the case of auto encoders, the input and the output domains are the same ( typically ). The Wikipedia page for Autoencoder, mentions,

> The simplest way to perform the copying task perfectly would be to duplicate the signal. Instead, autoencoders are typically forced to reconstruct the input approximately, preserving only the most relevant aspects of the data in the copy.

For instance, they are used for denoising data. The inputs for such a model are the noisy inputs and the outputs are the denoised inputs. You can find an [example](https://blog.keras.io/building-autoencoders-in-keras.html) here.

Encoder-Decoder models can also have different input and output domains, like in the case of [neural machine translation](https://en.wikipedia.org/wiki/Neural_machine_translation). The inputs and the outputs have different domains, as they belong to two different languages and can have variable lengths as well.



## Encoder kinds

cross-encoder

![image-20260206112746286](../assets/images/image-20260206112746286.png)

bi-encoder

![Screenshot 2026-02-06 at 11.28.02 AM](../assets/images/Screenshot 2026-02-06 at 11.28.02 AM.png)



## Training of Language Models

There are two main approaches for training a transformer model:

1. **Masked language modeling (MLM)**: Used by encoder models like BERT, this approach randomly masks some tokens in the input and trains the model to predict the original tokens based on the surrounding context. This allows the model to learn bidirectional context (looking at words both before and after the masked word).
2. **Causal language modeling (CLM)**: Used by decoder models like GPT, this approach predicts the next token based on all previous tokens in the sequence. The model can only use context from the left (previous tokens) to predict the next token.



## [large-scale expert parallelism](https://developer.nvidia.com/blog/scaling-large-moe-models-with-wide-expert-parallelism-on-nvl72-rack-scale-systems/)



## [Disaggregated Serving](https://www.nvidia.com/en-us/glossary/disaggregated-serving/)



https://nvdam.widen.net/s/fdmgprlmjv/disaggregated-inference-infographic?mkt_tok=MTU2LU9GTi03NDIAAAGdboFRquWk_jXCT6AGaUIXyJM6LtW6jLYlRypfYWFjhvhXDcmeFRFPpMLMajcHOYAC10dWgW5QuvSAI4avyvYI





## [supervised fine-tuning](https://cloud.google.com/blog/products/ai-machine-learning/supervised-fine-tuning-for-gemini-llm)



## [Low-Rank Adaptation of Large Language Models (LoRA)](https://arxiv.org/abs/2106.09685) 







## Logits

Logits are a fundamental concept in the field of artificial intelligence (AI) and machine learning, particularly within the architecture of neural networks. They **represent the raw, unnormalized scores produced by the last layer of a neural network before these scores are transformed into probabilities**.



## Chips

### NVIDIA Rubin CPX

In the context of the NVIDIA Rubin architecture, **CPX** stands for **Context Phase Accelerator**.

The name is a direct reference to the specific part of the AI inference lifecycle the chip is designed to handle. To understand why NVIDIA chose this name, it helps to look at the two distinct "phases" of running a Large Language Model (LLM):

#### 1. The Context Phase (Prefill)

This is the "input" stage. When you provide a prompt—especially a very long one involving entire books, massive codebases, or hours of video—the GPU must process all that existing data simultaneously to understand the "context."

- **Performance Bottleneck:** This phase is **Compute-Bound**. It requires massive mathematical throughput to calculate the "attention" scores across all input tokens at once.
- **The CPX Role:** The **Context Phase Accelerator** is packed with extra math units (ALUs) and hardware-based attention engines to chew through these massive inputs as fast as possible.

#### 2. The Generation Phase (Decode)

This is the "output" stage where the AI generates text token-by-token.

- **Performance Bottleneck:** This phase is **Memory-Bandwidth Bound**. The GPU has to wait for data to travel from the memory (HBM) to the processor for every single word it generates.
- **The Standard Rubin Role:** The standard "Big" Rubin GPU, equipped with HBM4, handles this part because it has the extreme memory speeds required to keep the generation fluid.

------

### Comparison: Why "Context" vs. Standard

### Historical Context

While "CPX" has been used in older technology (like "Cluster Processor eXtension" in some DSPs or "Compact PCI Express" in hardware standards), in the world of 2026 AI infrastructure, it is strictly NVIDIA's branding for **disaggregated inference**. By separating the "Context" processing from the "Generation" processing, they can build more efficient, specialized data centers.

Would you like to see how the CPX compares to the previous Blackwell generation in terms of total cost per million tokens?

## References

[LLM Embeddings Explained: A Visual and Intuitive Guide](https://huggingface.co/spaces/hesamation/primer-llm-embedding)

[A Visual Guide to Quantization](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization)

[GPUMode Lecture 58: Disaggregated LLM Inference](https://www.youtube.com/watch?v=tIPDwUepXcA)

[LLM Inference Handbook (BentoML)](https://bentoml.com/llm/)

[Mastering LLM Techniques: Inference Optimization](https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/)



